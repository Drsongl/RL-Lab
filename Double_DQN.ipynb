{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Double DQN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent1rookie/RL-Lab/blob/master/Double_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMk98MlbF8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "class DoubleQNetworkAgent:\n",
        "\n",
        "    def __init__(self, name: str, n_action: int, n_obs: int, units_layer: tuple,\n",
        "                 learning_rate=1e-4, gamma=0.99, seed=1,\n",
        "                 epsilon_init=0, epsilon_increase=0.003, epsilon_max=0.95,\n",
        "                 buffer_size=50000, batch_size=32, target_change_step=200, min_buffer_size=10000,\n",
        "                 save_path=None, load_path=None):\n",
        "        \"\"\"\n",
        "        To initialize an agent based on double deep Q learning\n",
        "        :param name: name of model\n",
        "        :param n_action: dimension of action space\n",
        "        :param n_obs: dimension of state space\n",
        "        :param units_layer: number of hidden layers and numbers of units in each layer\n",
        "        :param learning_rate: learning rate of optimizer\n",
        "        :param gamma: reward discount\n",
        "        :param seed: random seed\n",
        "        :param epsilon_init: initial epsilon value for epsilon-greedy policy\n",
        "        :param epsilon_increase: increase step for epsilon after each certain learning step\n",
        "        :param epsilon_max: upper threshold for epsilon\n",
        "        :param buffer_size: refers maximum size of transitions in buffer\n",
        "        :param batch_size: transition used in each training step\n",
        "        :param target_change_step: length of period for each refresh of Target Q-Network\n",
        "        :param min_buffer_size: minimum transitions for training process to initiate\n",
        "        :param save_path: path to save model variables\n",
        "        :param load_path: path to reload model variables\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.n_action = n_action\n",
        "        self.n_obs = n_obs\n",
        "        self.n_layer = len(units_layer)\n",
        "        self.gamma = gamma\n",
        "        # self.tau = tau\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon_init\n",
        "        self.epsilon_increase = epsilon_increase\n",
        "        self.epsilon_max = epsilon_max\n",
        "        self.initializer = tf.contrib.layers.xavier_initializer(seed=seed)\n",
        "        self.units_layer = (self.n_obs,) + units_layer + (self.n_action,)\n",
        "        self.sess = tf.Session()\n",
        "        self.transition_count = 0\n",
        "        self.buffer_size = buffer_size\n",
        "        self.min_buffer_size = min_buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.target_change_step = target_change_step\n",
        "        self.buffer_list = []\n",
        "        self.learn_step_count = 0\n",
        "        self.l_history = []\n",
        "\n",
        "        with tf.variable_scope(self.name + '_input', reuse=tf.AUTO_REUSE):\n",
        "            self.s = tf.placeholder(tf.float32, [None, self.n_obs], name='s')\n",
        "            self.s_next = tf.placeholder(tf.float32, [None, self.n_obs], name='s_next')\n",
        "            self.a = tf.placeholder(tf.int32, name='a')\n",
        "            self.r = tf.placeholder(tf.float32, name='r')\n",
        "            self.d = tf.placeholder(tf.float32, name='d')\n",
        "\n",
        "        with tf.variable_scope(self.name + '_main_params', reuse=tf.AUTO_REUSE):\n",
        "            self.W_main, self.b_main = [], []\n",
        "            for i in range(self.n_layer + 1):\n",
        "                self.W_main.append(tf.get_variable('W' + str(i),\n",
        "                                                   [self.units_layer[i], self.units_layer[i + 1]],\n",
        "                                                   initializer=self.initializer))\n",
        "                self.b_main.append(tf.get_variable('b' + str(i),\n",
        "                                                   [1, self.units_layer[i + 1]],\n",
        "                                                   initializer=self.initializer))\n",
        "\n",
        "        with tf.variable_scope(self.name + '_main_layers', reuse=tf.AUTO_REUSE):\n",
        "            self.layer_main = [self.s]\n",
        "            for i in range(self.n_layer + 1):\n",
        "                if i < self.n_layer:\n",
        "                    self.layer_main.append(\n",
        "                        tf.nn.relu(tf.add(tf.matmul(self.layer_main[i], self.W_main[i]), self.b_main[i])))\n",
        "                else:\n",
        "                    self.layer_main.append(\n",
        "                        tf.add(tf.matmul(self.layer_main[i], self.W_main[i]), self.b_main[i]))\n",
        "\n",
        "        with tf.variable_scope(self.name + '_target_params', reuse=tf.AUTO_REUSE):\n",
        "            self.W_target, self.b_target = [], []\n",
        "            for i in range(self.n_layer + 1):\n",
        "                self.W_target.append(tf.Variable(self.W_main[i].initialized_value(), name='W' + str(i)))\n",
        "                self.b_target.append(tf.Variable(self.b_main[i].initialized_value(), name='b' + str(i)))\n",
        "\n",
        "        with tf.variable_scope(self.name + '_target_layers', reuse=tf.AUTO_REUSE):\n",
        "            self.layer_target = [self.s_next]\n",
        "            for i in range(self.n_layer + 1):\n",
        "                if i < self.n_layer:\n",
        "                    self.layer_target.append(\n",
        "                        tf.nn.relu(tf.add(tf.matmul(self.layer_target[i], self.W_target[i]), self.b_target[i])))\n",
        "                else:\n",
        "                    self.layer_target.append(\n",
        "                        tf.add(tf.matmul(self.layer_target[i], self.W_target[i]), self.b_target[i]))\n",
        "\n",
        "        with tf.variable_scope(self.name + '_loss', reuse=tf.AUTO_REUSE):\n",
        "            # Choose best action based on main Q-Network\n",
        "            self.action_next_one_hot = tf.one_hot(tf.argmax(self.layer_main[-1], axis=1), self.n_action)\n",
        "            self.action_one_hot = tf.one_hot(self.a, self.n_action)\n",
        "            self.Q_real = tf.reduce_sum(self.layer_target[-1] * self.action_next_one_hot, axis=1) * self.gamma * (\n",
        "                        tf.ones_like(self.d) - self.d) + self.r\n",
        "            self.Q_eval = tf.reduce_sum(self.layer_main[-1] * self.action_one_hot, axis=1)\n",
        "            self.td_error = tf.square(self.Q_eval - self.Q_real)\n",
        "            self.loss = tf.reduce_mean(self.td_error)\n",
        "\n",
        "        with tf.variable_scope(self.name + '_op', reuse=tf.AUTO_REUSE):\n",
        "            self.op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "\n",
        "        # with tf.variable_scope(self.name + 'target_renew', reuse=tf.AUTO_REUSE):\n",
        "        #     for i in range(self.n_layer+1):\n",
        "        #         self.sess.run(self.W_target[i] += self.tau * (self.W_main[i] - self.W_target[i]))\n",
        "        #         self.sess.run(self.b_target[i] += self.tau * (self.b_main[i] - self.b_target[i]))\n",
        "\n",
        "        # Try to save and/or reload model\n",
        "        self.save_path = save_path\n",
        "        self.saver = tf.train.Saver()\n",
        "        if load_path is not None:\n",
        "            self.load_path = load_path\n",
        "            self.saver.restore(self.sess, self.load_path)\n",
        "        else:\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def act(self, obs, test_mode=False):\n",
        "        \"\"\"\n",
        "        To make an action when received a new observation based on epsilon-greedy policy.\n",
        "\n",
        "        :param obs: observation\n",
        "        :param test_mode: if True, will not apply epsilon-greedy policy\n",
        "        :return: an int indicating corresponding action\n",
        "        \"\"\"\n",
        "        if np.random.uniform() < self.epsilon or test_mode:\n",
        "            return np.argmax(self.sess.run(self.layer_main[-1], feed_dict={self.s: obs.reshape([1, self.n_obs])}))\n",
        "        else:\n",
        "            return np.random.randint(0, self.n_action)\n",
        "\n",
        "    def record(self, state: np.ndarray, action: int, reward: float, state_next: np.ndarray, done: bool):\n",
        "        \"\"\"\n",
        "        To record a transition of (s,a,r,s_next,done)  into replay buffer\n",
        "        \"\"\"\n",
        "        transition = np.hstack([state.reshape([1, self.n_obs]), np.array([action, reward, done]).reshape([1, 3]),\n",
        "                                state_next.reshape([1, self.n_obs])])\n",
        "        if self.transition_count < self.buffer_size:\n",
        "            self.buffer_list.append(transition)\n",
        "        else:\n",
        "            index = self.transition_count % self.buffer_size\n",
        "            self.buffer_list[index] = transition\n",
        "        self.transition_count += 1\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        To train the model after each transition\n",
        "        \"\"\"\n",
        "        if len(self.buffer_list) <= self.min_buffer_size:\n",
        "            return\n",
        "        # Sample from buffer\n",
        "        sample = np.vstack(random.sample(self.buffer_list, self.batch_size))\n",
        "        s = sample[:, :self.n_obs]\n",
        "        a = sample[:, self.n_obs:self.n_obs + 1].flatten()\n",
        "        r = sample[:, self.n_obs + 1: self.n_obs + 2].flatten()\n",
        "        d = sample[:, self.n_obs + 2: self.n_obs + 3].flatten()\n",
        "        s_next = sample[:, self.n_obs + 3:]\n",
        "\n",
        "        # Train the main DQN and record loss\n",
        "        self.sess.run(self.op, feed_dict={self.s: s, self.a: a, self.r: r, self.s_next: s_next, self.d: d})\n",
        "\n",
        "        self.l_history.append(\n",
        "            self.sess.run(self.loss, feed_dict={self.s: s, self.a: a, self.r: r, self.s_next: s_next, self.d: d}))\n",
        "\n",
        "        # Count learning step and print current loss,  Increase epsilon\n",
        "        self.learn_step_count += 1\n",
        "        if self.learn_step_count % 5 == 0:\n",
        "            self.epsilon = self.epsilon + self.epsilon_increase if self.epsilon < self.epsilon_max else self.epsilon_max\n",
        "\n",
        "        # Update Target DQN, another way is to replace target with main Q network after certain step.\n",
        "        if self.learn_step_count % self.target_change_step == 0:\n",
        "            for i in range(self.n_layer + 1):\n",
        "                self.sess.run(self.W_target[i].assign(self.W_main[i].value()))\n",
        "                self.sess.run(self.b_target[i].assign(self.b_main[i].value()))\n",
        "        # Another way to update Target Network\n",
        "        #         for i in range(self.n_layer + 1):\n",
        "        #             self.sess.run(self.W_target[i].assign(\n",
        "        #                 self.tau * self.W_main[i].value() + (1-self.tau) * self.W_target[i].value()))\n",
        "        #             self.sess.run(self.b_target[i].assign(\n",
        "        #                 self.tau * self.b_main[i].value() + (1-self.tau) * self.b_target[i].value()))\n",
        "        # self.W_target_value += self.tau * (self.sess.run(self.W_main) - self.W_target_value)\n",
        "        # self.b_target_value += self.tau * (self.sess.run(self.b_main) - self.b_target_value)\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"\n",
        "        To save the model\n",
        "        \"\"\"\n",
        "        if self.save_path is not None:\n",
        "            self.saver.save(self.sess, self.save_path)\n",
        "        else:\n",
        "            print(\"Save Path needed\")\n",
        "\n",
        "        #     def check(self):\n",
        "        #         sample = np.vstack(random.sample(self.buffer_list, self.batch_size))\n",
        "        #         s = sample[:, :self.n_obs]\n",
        "        #         a = sample[:, self.n_obs:self.n_obs+1].flatten()\n",
        "        #         r = sample[:, self.n_obs+1: self.n_obs+2].flatten()\n",
        "        #         d = sample[:, self.n_obs+2: self.n_obs+3].flatten()\n",
        "        #         s_next = sample[:, self.n_obs+3:]\n",
        "\n",
        "    def plot_cost(self):\n",
        "        \"\"\"\n",
        "        To print the loss change after each training episode\n",
        "        \"\"\"\n",
        "        plt.plot(np.arange(len(self.l_history)), self.l_history)\n",
        "        plt.ylabel('Cost')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtJfSCCkbvD_",
        "colab_type": "code",
        "outputId": "099641d7-bc18-47ba-b2a0-0ad458fc3994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install box2d-py\n",
        "!pip install gym[Box_2D]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n",
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 3.2MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "\u001b[33m  WARNING: gym 0.10.11 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box_2D]) (1.3.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box_2D]) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box_2D]) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box_2D]) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box_2D]) (2019.3.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeba0v8vclRU",
        "colab_type": "code",
        "outputId": "1cb8d3dc-ccf4-423d-9a5b-a1a0983125f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llKpBHm6c_U-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTxCtWxb1yJ",
        "colab_type": "code",
        "outputId": "d86b01f2-b6fb-4ded-d83c-fc1c26e323de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4369
        }
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Policy gradient has high variance, seed for reproducability\n",
        "env.seed(1)\n",
        "\n",
        "print(\"env.action_space\", env.action_space)\n",
        "print(\"env.observation_space\", env.observation_space)\n",
        "print(\"env.observation_space.high\", env.observation_space.high)\n",
        "print(\"env.observation_space.low\", env.observation_space.low)\n",
        "RENDER_ENV = True\n",
        "EPISODES = 5000\n",
        "RENDER_REWARD_MIN = 5000\n",
        "\n",
        "\n",
        "# Load checkpoint\n",
        "load_version = 0\n",
        "save_version = load_version + 1\n",
        "# load_path = \"LunarLander-v2.ckpt\"\n",
        "save_path = \"LunarLander-v3.ckpt\"\n",
        "reward_list = []\n",
        "\n",
        "Agent = DoubleQNetworkAgent(\n",
        "    n_obs=env.observation_space.shape[0],\n",
        "    n_action=env.action_space.n,\n",
        "    units_layer=(64,64),\n",
        "    learning_rate=0.0025,\n",
        "    name='ll11',\n",
        "    gamma=0.99,\n",
        "    load_path=None,\n",
        "    save_path=save_path\n",
        ")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env.action_space Discrete(4)\n",
            "env.observation_space Box(8,)\n",
            "env.observation_space.high [inf inf inf inf inf inf inf inf]\n",
            "env.observation_space.low [-inf -inf -inf -inf -inf -inf -inf -inf]\n",
            "Reward for episode 0 is -110.390538\n",
            "Reward for episode 1 is -232.632292\n",
            "Reward for episode 2 is -110.356303\n",
            "Reward for episode 3 is -48.682760\n",
            "Reward for episode 4 is -250.625135\n",
            "Reward for episode 5 is -94.794422\n",
            "Reward for episode 6 is -91.275395\n",
            "Reward for episode 7 is -80.824549\n",
            "Reward for episode 8 is -39.442133\n",
            "Reward for episode 9 is -137.568172\n",
            "Reward for episode 10 is -75.385483\n",
            "Reward for episode 11 is -437.636948\n",
            "Reward for episode 12 is -142.462380\n",
            "Reward for episode 13 is -121.976945\n",
            "Reward for episode 14 is -383.578949\n",
            "Reward for episode 15 is 14.762192\n",
            "Reward for episode 16 is -111.251218\n",
            "Reward for episode 17 is -90.018715\n",
            "Reward for episode 18 is -106.034009\n",
            "Reward for episode 19 is -163.481745\n",
            "Reward for episode 20 is -153.557432\n",
            "Reward for episode 21 is -124.016054\n",
            "Reward for episode 22 is -41.149564\n",
            "Reward for episode 23 is -211.389641\n",
            "Reward for episode 24 is -99.166418\n",
            "Reward for episode 25 is -73.382616\n",
            "Reward for episode 26 is -137.659835\n",
            "Reward for episode 27 is -359.070733\n",
            "Reward for episode 28 is -509.778058\n",
            "Reward for episode 29 is -119.074998\n",
            "Reward for episode 30 is -276.624604\n",
            "Reward for episode 31 is -156.203157\n",
            "Reward for episode 32 is -70.258578\n",
            "Reward for episode 33 is -101.173435\n",
            "Reward for episode 34 is -191.705897\n",
            "Reward for episode 35 is -239.377745\n",
            "Reward for episode 36 is -431.744127\n",
            "Reward for episode 37 is -428.530044\n",
            "Reward for episode 38 is -275.910027\n",
            "Reward for episode 39 is -268.946807\n",
            "Reward for episode 40 is -206.148258\n",
            "Reward for episode 41 is -115.996067\n",
            "Reward for episode 42 is -68.148774\n",
            "Reward for episode 43 is -171.044735\n",
            "Reward for episode 44 is -135.600236\n",
            "Reward for episode 45 is -297.377846\n",
            "Reward for episode 46 is -287.200613\n",
            "Reward for episode 47 is -320.545327\n",
            "Reward for episode 48 is -117.237704\n",
            "Reward for episode 49 is -73.568370\n",
            "Reward for episode 50 is -127.499913\n",
            "Reward for episode 51 is -125.357322\n",
            "Reward for episode 52 is -88.321735\n",
            "Reward for episode 53 is -112.523914\n",
            "Reward for episode 54 is -138.960546\n",
            "Reward for episode 55 is -203.623182\n",
            "Reward for episode 56 is -375.885275\n",
            "Reward for episode 57 is -192.867952\n",
            "Reward for episode 58 is -313.775205\n",
            "Reward for episode 59 is -119.494886\n",
            "Reward for episode 60 is -161.284359\n",
            "Reward for episode 61 is -289.811010\n",
            "Reward for episode 62 is -285.097714\n",
            "Reward for episode 63 is -378.543722\n",
            "Reward for episode 64 is -208.548580\n",
            "Reward for episode 65 is -277.706033\n",
            "Reward for episode 66 is -215.246933\n",
            "Reward for episode 67 is -83.194621\n",
            "Reward for episode 68 is -242.735537\n",
            "Reward for episode 69 is -425.926787\n",
            "Reward for episode 70 is -225.980283\n",
            "Reward for episode 71 is -168.874864\n",
            "Reward for episode 72 is -93.311773\n",
            "Reward for episode 73 is -112.525110\n",
            "Reward for episode 74 is -59.653931\n",
            "Reward for episode 75 is -72.077847\n",
            "Reward for episode 76 is -209.008431\n",
            "Reward for episode 77 is -240.225000\n",
            "Reward for episode 78 is -143.214460\n",
            "Reward for episode 79 is -122.106126\n",
            "Reward for episode 80 is -303.873115\n",
            "Reward for episode 81 is -387.069880\n",
            "Reward for episode 82 is -403.802970\n",
            "Reward for episode 83 is -330.223112\n",
            "Reward for episode 84 is -447.837188\n",
            "Reward for episode 85 is -81.800484\n",
            "Reward for episode 86 is -105.595488\n",
            "Reward for episode 87 is -219.362307\n",
            "Reward for episode 88 is -127.286738\n",
            "Reward for episode 89 is -154.409440\n",
            "Reward for episode 90 is -142.059836\n",
            "Reward for episode 91 is -52.304080\n",
            "Reward for episode 92 is -349.006246\n",
            "Reward for episode 93 is -323.610839\n",
            "Reward for episode 94 is -288.350311\n",
            "Reward for episode 95 is -36.196954\n",
            "Reward for episode 96 is -210.497982\n",
            "Reward for episode 97 is -82.901532\n",
            "Reward for episode 98 is -113.274413\n",
            "Reward for episode 99 is -79.621964\n",
            "Reward for episode 100 is -123.557926\n",
            "Reward for episode 101 is -184.729665\n",
            "Reward for episode 102 is -107.819280\n",
            "Reward for episode 103 is -297.186022\n",
            "Reward for episode 104 is -438.581053\n",
            "Reward for episode 105 is -199.848655\n",
            "Reward for episode 106 is -187.040587\n",
            "Reward for episode 107 is -117.458295\n",
            "Reward for episode 108 is -148.450033\n",
            "Reward for episode 109 is -96.319094\n",
            "Reward for episode 110 is -68.534243\n",
            "Reward for episode 111 is -127.466122\n",
            "Reward for episode 112 is -81.725424\n",
            "Reward for episode 113 is -58.554198\n",
            "Reward for episode 114 is -3.980109\n",
            "Reward for episode 115 is -233.226403\n",
            "Reward for episode 116 is -219.052649\n",
            "Reward for episode 117 is -105.620396\n",
            "Reward for episode 118 is -123.137615\n",
            "Reward for episode 119 is -316.274292\n",
            "Reward for episode 120 is -253.111258\n",
            "Reward for episode 121 is -360.202161\n",
            "Reward for episode 122 is -131.461074\n",
            "Reward for episode 123 is -88.830247\n",
            "Reward for episode 124 is -376.921074\n",
            "Reward for episode 125 is -228.701765\n",
            "Reward for episode 126 is -523.739208\n",
            "Reward for episode 127 is -106.782141\n",
            "Reward for episode 128 is -102.852636\n",
            "Reward for episode 129 is -276.460802\n",
            "Reward for episode 130 is -225.740655\n",
            "Reward for episode 131 is -202.272228\n",
            "Reward for episode 132 is -210.216004\n",
            "Reward for episode 133 is -392.946065\n",
            "Reward for episode 134 is -227.081316\n",
            "Reward for episode 135 is -96.587202\n",
            "Reward for episode 136 is -87.050203\n",
            "Reward for episode 137 is 75.183256\n",
            "Reward for episode 138 is -280.992403\n",
            "Reward for episode 139 is -285.120293\n",
            "Reward for episode 140 is -147.981794\n",
            "Reward for episode 141 is -223.229058\n",
            "Reward for episode 142 is -261.797628\n",
            "Reward for episode 143 is -227.988054\n",
            "Reward for episode 144 is 84.686957\n",
            "Reward for episode 145 is -171.793181\n",
            "Reward for episode 146 is -330.440421\n",
            "Reward for episode 147 is -112.985832\n",
            "Reward for episode 148 is -189.722671\n",
            "Reward for episode 149 is -270.018320\n",
            "Reward for episode 150 is -227.233226\n",
            "Reward for episode 151 is -245.406137\n",
            "Reward for episode 152 is -129.995444\n",
            "Reward for episode 153 is -224.817738\n",
            "Reward for episode 154 is -69.514368\n",
            "Reward for episode 155 is -126.861669\n",
            "Reward for episode 156 is -55.259733\n",
            "Reward for episode 157 is -126.266290\n",
            "Reward for episode 158 is -185.278050\n",
            "Reward for episode 159 is -470.612683\n",
            "Reward for episode 160 is -168.849165\n",
            "Reward for episode 161 is -409.347744\n",
            "Reward for episode 162 is -254.955139\n",
            "Reward for episode 163 is -486.159737\n",
            "Reward for episode 164 is -486.889950\n",
            "Reward for episode 165 is -177.380327\n",
            "Reward for episode 166 is 32.086540\n",
            "Reward for episode 167 is -550.352654\n",
            "Reward for episode 168 is -303.440837\n",
            "Reward for episode 169 is -142.081000\n",
            "Reward for episode 170 is -204.887849\n",
            "Reward for episode 171 is -412.551707\n",
            "Reward for episode 172 is -327.251419\n",
            "Reward for episode 173 is -123.582721\n",
            "Reward for episode 174 is -520.964537\n",
            "Reward for episode 175 is -213.429449\n",
            "Reward for episode 176 is -212.634715\n",
            "Reward for episode 177 is -425.097389\n",
            "Reward for episode 178 is -160.358559\n",
            "Reward for episode 179 is -285.933091\n",
            "Reward for episode 180 is -233.257023\n",
            "Reward for episode 181 is -155.178756\n",
            "Reward for episode 182 is -200.333768\n",
            "Reward for episode 183 is -204.758054\n",
            "Reward for episode 184 is -299.014271\n",
            "Reward for episode 185 is -172.171350\n",
            "Reward for episode 186 is -159.342953\n",
            "Reward for episode 187 is -193.729111\n",
            "Reward for episode 188 is -108.467631\n",
            "Reward for episode 189 is -193.459233\n",
            "Reward for episode 190 is -400.795519\n",
            "Reward for episode 191 is 49.942436\n",
            "Reward for episode 192 is -139.117725\n",
            "Reward for episode 193 is -164.019949\n",
            "Reward for episode 194 is -160.200475\n",
            "Reward for episode 195 is -270.369699\n",
            "Reward for episode 196 is -191.740393\n",
            "Reward for episode 197 is -129.612862\n",
            "Reward for episode 198 is -163.014408\n",
            "Reward for episode 199 is -105.592115\n",
            "Reward for episode 200 is 30.795381\n",
            "Reward for episode 201 is -162.870717\n",
            "Reward for episode 202 is -135.363287\n",
            "Reward for episode 203 is -60.381485\n",
            "Reward for episode 204 is -171.676622\n",
            "Reward for episode 205 is -150.989821\n",
            "Reward for episode 206 is -124.916476\n",
            "Reward for episode 207 is -115.075607\n",
            "Reward for episode 208 is -149.739793\n",
            "Reward for episode 209 is -172.885464\n",
            "Reward for episode 210 is -129.340742\n",
            "Reward for episode 211 is -125.251268\n",
            "Reward for episode 212 is -127.944021\n",
            "Reward for episode 213 is -143.644104\n",
            "Reward for episode 214 is -132.915688\n",
            "Reward for episode 215 is -149.957610\n",
            "Reward for episode 216 is -162.426922\n",
            "Reward for episode 217 is -145.870187\n",
            "Reward for episode 218 is -159.372008\n",
            "Reward for episode 219 is -135.224319\n",
            "Reward for episode 220 is -162.455063\n",
            "Reward for episode 221 is 179.542859\n",
            "Reward for episode 222 is -106.593083\n",
            "Reward for episode 223 is -155.532546\n",
            "Reward for episode 224 is -181.237801\n",
            "Reward for episode 225 is -189.151349\n",
            "Reward for episode 226 is -210.586438\n",
            "Reward for episode 227 is -172.901862\n",
            "Reward for episode 228 is -126.276369\n",
            "Reward for episode 229 is -150.750047\n",
            "Reward for episode 230 is -132.727970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-86e5104edf3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-576a9aa06aec>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_step_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_change_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Another way to update Target Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxxQnPOjwvST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "c557110d-a316-4378-b868-6ae66d5358bb"
      },
      "source": [
        "for episode in range(500):\n",
        "\n",
        "    observation = env.reset()\n",
        "    episode_reward = 0\n",
        "    while True:\n",
        "        action = Agent.act(observation)\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        Agent.record(observation, action, reward, state, done)\n",
        "        Agent.learn()\n",
        "\n",
        "        observation = state\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "\n",
        "        if done:\n",
        "            print('Reward for episode %d is %f' %(episode, episode_reward))  \n",
        "            reward_list.append(episode_reward)\n",
        "            break\n",
        "\n",
        "            if max_reward_so_far > RENDER_REWARD_MIN: RENDER_ENV = True\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward for episode 0 is -161.466029\n",
            "Reward for episode 1 is -397.501300\n",
            "Reward for episode 2 is -132.043087\n",
            "Reward for episode 3 is 114.165877\n",
            "Reward for episode 4 is -157.933776\n",
            "Reward for episode 5 is -80.052862\n",
            "Reward for episode 6 is -129.634734\n",
            "Reward for episode 7 is -136.982849\n",
            "Reward for episode 8 is -163.601296\n",
            "Reward for episode 9 is -225.619267\n",
            "Reward for episode 10 is -151.158913\n",
            "Reward for episode 11 is -116.733145\n",
            "Reward for episode 12 is -144.720119\n",
            "Reward for episode 13 is -304.018293\n",
            "Reward for episode 14 is -137.434022\n",
            "Reward for episode 15 is -120.391993\n",
            "Reward for episode 16 is -69.473760\n",
            "Reward for episode 17 is -467.701330\n",
            "Reward for episode 18 is -160.845100\n",
            "Reward for episode 19 is -185.680994\n",
            "Reward for episode 20 is -177.423559\n",
            "Reward for episode 21 is -116.188507\n",
            "Reward for episode 22 is -51.239536\n",
            "Reward for episode 23 is -83.274878\n",
            "Reward for episode 24 is 236.115070\n",
            "Reward for episode 25 is -130.293501\n",
            "Reward for episode 26 is -117.979601\n",
            "Reward for episode 27 is -135.730067\n",
            "Reward for episode 28 is -183.776035\n",
            "Reward for episode 29 is -90.798454\n",
            "Reward for episode 30 is -132.626746\n",
            "Reward for episode 31 is -124.661354\n",
            "Reward for episode 32 is -252.370881\n",
            "Reward for episode 33 is -87.670116\n",
            "Reward for episode 34 is -351.957524\n",
            "Reward for episode 35 is -42.648061\n",
            "Reward for episode 36 is -101.180726\n",
            "Reward for episode 37 is -122.980018\n",
            "Reward for episode 38 is -123.286862\n",
            "Reward for episode 39 is -96.074193\n",
            "Reward for episode 40 is -79.448140\n",
            "Reward for episode 41 is -18.966652\n",
            "Reward for episode 42 is -77.750504\n",
            "Reward for episode 43 is -74.073201\n",
            "Reward for episode 44 is 137.760057\n",
            "Reward for episode 45 is -61.159744\n",
            "Reward for episode 46 is -61.257139\n",
            "Reward for episode 47 is -92.113713\n",
            "Reward for episode 48 is -121.394532\n",
            "Reward for episode 49 is -60.687323\n",
            "Reward for episode 50 is -39.742003\n",
            "Reward for episode 51 is -16.539436\n",
            "Reward for episode 52 is -24.727334\n",
            "Reward for episode 53 is -49.402238\n",
            "Reward for episode 54 is 170.476092\n",
            "Reward for episode 55 is 124.977537\n",
            "Reward for episode 56 is -58.447013\n",
            "Reward for episode 57 is -29.770411\n",
            "Reward for episode 58 is 193.555913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT6SE3BFpQo8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "54f88bf1-2c5a-4f19-8b5d-31ef79aba976"
      },
      "source": [
        "Agent.save_path = 'LunarLander-v2.ckpt'\n",
        "\n",
        "Agent = DoubleQNetworkAgent(\n",
        "    n_obs=env.observation_space.shape[0],\n",
        "    n_action=env.action_space.n,\n",
        "    units_layer=(64,64),\n",
        "    learning_rate=0.0025,\n",
        "    name='ll11',\n",
        "    gamma=0.99,\n",
        "    load_path= 'LunarLander-v2.ckpt',\n",
        "    save_path= 'LunarLander-v2.ckpt'\n",
        ")\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-572f69064fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'LunarLander-v2.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'LunarLander-v2.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-576a9aa06aec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, n_action, n_obs, units_layer, learning_rate, gamma, seed, epsilon_init, epsilon_increase, epsilon_max, buffer_size, batch_size, target_change_step, min_buffer_size, save_path, load_path)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[0;32m-> 1268\u001b[0;31m                        + compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: LunarLander-v2.ckpt"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0HSsBK7OJ5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Agent.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5XPvyBtk_0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qqq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4vQkxyzidYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kkk.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k90qDZbT_TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.hstack([observation.reshape([1,8]), np.array([action, reward]).reshape([1,2]), state.reshape([1,8])]).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S04-HGKdexoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample[:, 7:8].shape"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}